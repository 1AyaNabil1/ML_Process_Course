{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:11:23.076983Z","iopub.execute_input":"2022-10-13T01:11:23.077391Z","iopub.status.idle":"2022-10-13T01:11:23.096669Z","shell.execute_reply.started":"2022-10-13T01:11:23.077360Z","shell.execute_reply":"2022-10-13T01:11:23.095461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering for Continuous Variables \nIn this notebook we will cover scaling, transformations, and interactive features. This notebook is the This is a companion workbook for the 365 Data Science course on ML Process. The in-depth explanantions theories and pros/cons for each of these techniques can be found there. \n\n## Feature Scaling\nFeature scaling is important for we are using models with a distance metric. If our features are of different scales, they can be overcompensated for in the models. \n- Absolute Max Scaling\n- MinMax Scaling\n- Z-Score Normalization (Standard Scaler)\n- Robust Scaler \n## Transformations \n- Logarithmic \n- Square Root \n- Exponential\n- Box-Cox\n## Interaction Features\n- Arethmetic Interaction\n- Binning\n- Creative Features \n\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/craigslist-carstrucks-data/vehicles.csv')\n\n#let's add a column for car age that will help us later on: \ndf['car_age'] = df['year'].max() - df['year']","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:11:23.218066Z","iopub.execute_input":"2022-10-13T01:11:23.218961Z","iopub.status.idle":"2022-10-13T01:12:04.901892Z","shell.execute_reply.started":"2022-10-13T01:11:23.218911Z","shell.execute_reply":"2022-10-13T01:12:04.900665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:04.903672Z","iopub.execute_input":"2022-10-13T01:12:04.904211Z","iopub.status.idle":"2022-10-13T01:12:04.914970Z","shell.execute_reply.started":"2022-10-13T01:12:04.904177Z","shell.execute_reply":"2022-10-13T01:12:04.913569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()\n# Columns we may want to normalize \n# Price, Year, Odometer","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:04.916360Z","iopub.execute_input":"2022-10-13T01:12:04.916815Z","iopub.status.idle":"2022-10-13T01:12:05.147734Z","shell.execute_reply.started":"2022-10-13T01:12:04.916784Z","shell.execute_reply":"2022-10-13T01:12:05.146459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's just use a few features to create an example model and remove Nulls. Learn mnore about different imputation techniques in this other companion notebook. \n#pd.get_dummie() creates dummy variables for the categorical features (see this notebook for more on that)\ndf_example = pd.get_dummies(df.loc[:,['price','car_age','odometer','manufacturer','condition']].dropna())\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:05.150948Z","iopub.execute_input":"2022-10-13T01:12:05.152001Z","iopub.status.idle":"2022-10-13T01:12:05.347368Z","shell.execute_reply.started":"2022-10-13T01:12:05.151952Z","shell.execute_reply":"2022-10-13T01:12:05.346072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_example.drop('price',axis =1 )\ny = df_example[['price']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:05.348852Z","iopub.execute_input":"2022-10-13T01:12:05.349305Z","iopub.status.idle":"2022-10-13T01:12:05.540343Z","shell.execute_reply.started":"2022-10-13T01:12:05.349270Z","shell.execute_reply":"2022-10-13T01:12:05.539055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.price.plot.box()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:05.542146Z","iopub.execute_input":"2022-10-13T01:12:05.542514Z","iopub.status.idle":"2022-10-13T01:12:05.785729Z","shell.execute_reply.started":"2022-10-13T01:12:05.542477Z","shell.execute_reply":"2022-10-13T01:12:05.784339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.price.plot.box()\ndf.car_age.plot.box()\n#df.odometer.plot.box()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:05.787331Z","iopub.execute_input":"2022-10-13T01:12:05.788388Z","iopub.status.idle":"2022-10-13T01:12:05.977220Z","shell.execute_reply.started":"2022-10-13T01:12:05.788339Z","shell.execute_reply":"2022-10-13T01:12:05.976288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.odometer.plot.box()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:05.978710Z","iopub.execute_input":"2022-10-13T01:12:05.979213Z","iopub.status.idle":"2022-10-13T01:12:06.144509Z","shell.execute_reply.started":"2022-10-13T01:12:05.979166Z","shell.execute_reply":"2022-10-13T01:12:06.143211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling\nFeature scaling is important for we are using models with a distance metric. If our features are of different scales, they can be overcompensated for in the models. \n- Absolute Max Scaling\n- MinMax Scaling\n- Z-Score Normalization (Standard Scaler)\n- Robust Scaler \n","metadata":{}},{"cell_type":"markdown","source":"## Absolute Maximum Scaling\nAbsolute maximum scaling will have you take the maximum value within the data and then divide the raw data by this absolute maximum value.\n\nFor absolute max scaling, this works best if our data doesn't have massive outliers. In this case, we would likely want to remove outliers from price and odometer. This also keeps the same distribution of the data. For absolute maximum scaling, let's do this on the year data for the cars. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler\n\n#Scale data \ndf_am = MaxAbsScaler().fit_transform(X_train)\n\n#convert to dataframe to see table\ndf_am = pd.DataFrame(df_am, columns = X_train.columns)\n\n#obvious problems with outliers regarding price & odometer ","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:06.146522Z","iopub.execute_input":"2022-10-13T01:12:06.147137Z","iopub.status.idle":"2022-10-13T01:12:06.298281Z","shell.execute_reply.started":"2022-10-13T01:12:06.147075Z","shell.execute_reply":"2022-10-13T01:12:06.297102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Min Max Scaling\nAnother simple form of scaling is called min max. Min Max scaling will scale all our data points between 0 and 1. Weâ€™d use the following formula to scale our data, where we subtract the min from the raw data and then divide it by the max minus the min. \n\nAgain, this approach is not robust to outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndf_min_max = MinMaxScaler().fit_transform(X_train)\ndf_min_max = pd.DataFrame(df_min_max, columns = X_train.columns)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:06.303795Z","iopub.execute_input":"2022-10-13T01:12:06.304230Z","iopub.status.idle":"2022-10-13T01:12:06.446461Z","shell.execute_reply.started":"2022-10-13T01:12:06.304192Z","shell.execute_reply":"2022-10-13T01:12:06.445296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Z Score Normalization (Standard Scaling)\n\nAnother approach is standardization which transforms the data into the z-score, where the mean is zero and the standard deviation is 1.\n\nThis approach is more robust to outliers, but still can have issues if outliers cause massive changes to standard deviation. However, this does assume a normal distribution which is inaccurate for some of our data (Year).","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ndf_std = X_train.copy()\n#only scale numeric varaibles in this case rather than the dummy variables for categories \ndf_std.loc[:,['car_age','odometer']] = StandardScaler().fit_transform(df_std.loc[:, ['car_age','odometer']])\ndf_std","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:06.448032Z","iopub.execute_input":"2022-10-13T01:12:06.448522Z","iopub.status.idle":"2022-10-13T01:12:06.565121Z","shell.execute_reply.started":"2022-10-13T01:12:06.448475Z","shell.execute_reply":"2022-10-13T01:12:06.564074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Robust Scaler\nWith Robust Scaler, weâ€™re subtracting the median and then scaling the column by the IQR.\n\nThis is the approach most robust to outliers that we will cover.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\ndf_rob = X_train.copy()\n#only scale numeric varaibles in this case rather than the dummy variables for categories \ndf_rob.loc[:,['car_age','odometer']] = RobustScaler().fit_transform(df_rob.loc[:, ['car_age','odometer']])\ndf_rob","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:06.566737Z","iopub.execute_input":"2022-10-13T01:12:06.567338Z","iopub.status.idle":"2022-10-13T01:12:06.691230Z","shell.execute_reply.started":"2022-10-13T01:12:06.567303Z","shell.execute_reply":"2022-10-13T01:12:06.690103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's do a simple exmaple where we compare results with the different features scaling techniques. We will remove the categorical data for this. \n\n#the model we will be using is K Nearest Neighbors which can use euclidean distance. \n\n#we will use year and odometer to predict price \n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n#noscaling \nneigh_am = KNeighborsRegressor(n_neighbors=3)\nneigh_am.fit(X_train.loc[:,['car_age','odometer']], y_train)\npred = neigh_am.predict(X_test.loc[:,['car_age','odometer']])\n\n#absolute max \nneigh_am = KNeighborsRegressor(n_neighbors=3)\nneigh_am.fit(df_am.loc[:,['car_age','odometer']], y_train)\nam_pred = neigh_am.predict(X_test.loc[:,['car_age','odometer']])\n\n#min max (should get same results)\nneigh_mm = KNeighborsRegressor(n_neighbors=3)\nneigh_mm.fit(df_min_max.loc[:,['car_age','odometer']], y_train)\nmm_pred = neigh_mm.predict(X_test.loc[:,['car_age','odometer']])\n\n#standard (z score)\nneigh_std = KNeighborsRegressor(n_neighbors=3)\nneigh_std.fit(df_std.loc[:,['car_age','odometer']], y_train)\nstd_pred = neigh_std.predict(X_test.loc[:,['car_age','odometer']])\n\n#robust scaler \nneigh_rob = KNeighborsRegressor(n_neighbors=3)\nneigh_rob.fit(df_rob.loc[:,['car_age','odometer']], y_train)\nrob_pred = neigh_rob.predict(X_test.loc[:,['car_age','odometer']])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:06.693197Z","iopub.execute_input":"2022-10-13T01:12:06.693667Z","iopub.status.idle":"2022-10-13T01:12:08.090994Z","shell.execute_reply.started":"2022-10-13T01:12:06.693624Z","shell.execute_reply":"2022-10-13T01:12:08.089790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('No Scaling: %.3f' % mean_absolute_error(y_test,pred))\nprint('Abosolute Max Score: %.3f' % mean_absolute_error(y_test,am_pred))\nprint('Min Max Score: %.3f' % mean_absolute_error(y_test,mm_pred))\nprint('Standard Scaling Score: %.3f' % mean_absolute_error(y_test,std_pred))\nprint('Robust Scaler Score: %.3f' % mean_absolute_error(y_test,rob_pred))\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:08.092502Z","iopub.execute_input":"2022-10-13T01:12:08.092850Z","iopub.status.idle":"2022-10-13T01:12:08.112358Z","shell.execute_reply.started":"2022-10-13T01:12:08.092819Z","shell.execute_reply":"2022-10-13T01:12:08.111124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformations \nA data transformation is the process of using a math expression to change the structure of our data. As we mentioned before, some models need data to fit a specific type of distribution for them to produce optimal results. Unfortunately, the data we get in the real world, doesnâ€™t always fit the distributions our models call for. \n\nLet's look at the shape of our data and if it has any outliers before we do our transforms","metadata":{}},{"cell_type":"code","source":"# visual of the distribution of the odometer without any outlier removal (see boxplots above)\n#data is clearly impacted heavily by outliers \nprint(\"max odometer: \" + str(df_example['odometer'].max()))\nprint(\"median odometer: \" + str(df_example['odometer'].median()))\n\ndf_example['odometer'].hist(bins=50)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:08.114527Z","iopub.execute_input":"2022-10-13T01:12:08.115068Z","iopub.status.idle":"2022-10-13T01:12:08.379396Z","shell.execute_reply.started":"2022-10-13T01:12:08.115006Z","shell.execute_reply":"2022-10-13T01:12:08.377972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data after very basic oultier removal (kept only data < 99th percentile)\n#clear right skew in data \ndf_example[df['odometer']<df['odometer'].quantile(.99)]['odometer'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:08.382203Z","iopub.execute_input":"2022-10-13T01:12:08.383375Z","iopub.status.idle":"2022-10-13T01:12:08.803184Z","shell.execute_reply.started":"2022-10-13T01:12:08.383329Z","shell.execute_reply":"2022-10-13T01:12:08.801940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visual of the distribution of the odometer without any outlier removal (see boxplots above)\n#data is clearly impacted heavily by outliers \nprint(\"max price: \" + str(df_example['price'].max()))\nprint(\"median price: \" + str(df_example['price'].median()))\n\ndf_example['price'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:08.804655Z","iopub.execute_input":"2022-10-13T01:12:08.804975Z","iopub.status.idle":"2022-10-13T01:12:09.139773Z","shell.execute_reply.started":"2022-10-13T01:12:08.804946Z","shell.execute_reply":"2022-10-13T01:12:09.138354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data after very basic oultier removal (kept only data < 99th percentile)\n#clear right skew in data \n\ndf_example[df['price']<df['price'].quantile(.99)]['price'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:12:09.141273Z","iopub.execute_input":"2022-10-13T01:12:09.141623Z","iopub.status.idle":"2022-10-13T01:12:09.569237Z","shell.execute_reply.started":"2022-10-13T01:12:09.141592Z","shell.execute_reply":"2022-10-13T01:12:09.567924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do some simple feature engineering to get how old the cars are\n\ndf_example['car_age'] = df_example['car_age'].max() - df_example['car_age']\n\nprint(\"max age: \" + str(df_example['car_age'].max()))\nprint(\"median age: \" + str(df_example['car_age'].median()))\n\ndf_example['car_age'].hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:25:03.891625Z","iopub.execute_input":"2022-10-13T01:25:03.892126Z","iopub.status.idle":"2022-10-13T01:25:04.216904Z","shell.execute_reply.started":"2022-10-13T01:25:03.892072Z","shell.execute_reply":"2022-10-13T01:25:04.215649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logarithmic Transformation\nA very popular, common type of transformation is the log transformation. Log transformations fall under the family of power transformations. Typically, we apply logarithmic transformations to our variables when our variables are heavily right skewed, driven by a few outliers. \n\nLet's see how these transformations impact some of our skewed data (Odometer & Price)\n\n### Transforms we will cover\n- Logarithmic \n- Exponential\n- Square Root \n- Box-Cox","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\ndef log_transform(x):\n    return np.log(x + 1)\n\ntransformer_log = FunctionTransformer(log_transform)\ntransformed_log = transformer_log.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:19:33.712995Z","iopub.execute_input":"2022-10-13T01:19:33.713418Z","iopub.status.idle":"2022-10-13T01:19:33.928337Z","shell.execute_reply.started":"2022-10-13T01:19:33.713386Z","shell.execute_reply":"2022-10-13T01:19:33.927319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_logp = FunctionTransformer(log_transform)\ntransformed_logp = transformer_logp.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:19:39.745476Z","iopub.execute_input":"2022-10-13T01:19:39.745875Z","iopub.status.idle":"2022-10-13T01:19:39.753365Z","shell.execute_reply.started":"2022-10-13T01:19:39.745842Z","shell.execute_reply":"2022-10-13T01:19:39.752131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as you can see, using log transform in this case actually creates some right skew. \n#It does however almost completely normalize the outliers that were present\n\ntransformed_log['odometer'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:19:39.929629Z","iopub.execute_input":"2022-10-13T01:19:39.930322Z","iopub.status.idle":"2022-10-13T01:19:40.358469Z","shell.execute_reply.started":"2022-10-13T01:19:39.930286Z","shell.execute_reply":"2022-10-13T01:19:40.357205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_log['car_age'].hist(bins = 20)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:28:11.096180Z","iopub.execute_input":"2022-10-13T01:28:11.096565Z","iopub.status.idle":"2022-10-13T01:28:11.364595Z","shell.execute_reply.started":"2022-10-13T01:28:11.096535Z","shell.execute_reply":"2022-10-13T01:28:11.363436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as you can see, using log transform in this case actually creates some right skew. \n#It does however almost completely normalize the outliers that were present\n\ntransformed_logp.hist(bins =100)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:19:43.640078Z","iopub.execute_input":"2022-10-13T01:19:43.640570Z","iopub.status.idle":"2022-10-13T01:19:44.208714Z","shell.execute_reply.started":"2022-10-13T01:19:43.640531Z","shell.execute_reply":"2022-10-13T01:19:44.207545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Square Root Transform\nSquare/Square Root transformations will compress the spread of your larger values but spread out your lower values. Log transformations have a similar effect but are much more aggressive","metadata":{}},{"cell_type":"code","source":"def sqrt_transform(x):\n    return np.sqrt(x)\n\ntransformer_sqrt = FunctionTransformer(sqrt_transform)\ntransformed_sqrt = transformer_sqrt.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:20:17.747207Z","iopub.execute_input":"2022-10-13T01:20:17.747610Z","iopub.status.idle":"2022-10-13T01:20:17.930618Z","shell.execute_reply.started":"2022-10-13T01:20:17.747580Z","shell.execute_reply":"2022-10-13T01:20:17.929572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_sqrtp = FunctionTransformer(sqrt_transform)\ntransformed_sqrtp = transformer_sqrtp.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:20:18.008556Z","iopub.execute_input":"2022-10-13T01:20:18.008943Z","iopub.status.idle":"2022-10-13T01:20:18.015099Z","shell.execute_reply.started":"2022-10-13T01:20:18.008914Z","shell.execute_reply":"2022-10-13T01:20:18.013905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrt['odometer'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:20:18.179468Z","iopub.execute_input":"2022-10-13T01:20:18.179879Z","iopub.status.idle":"2022-10-13T01:20:18.593652Z","shell.execute_reply.started":"2022-10-13T01:20:18.179843Z","shell.execute_reply":"2022-10-13T01:20:18.592474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrt['car_age'].hist(bins = 20)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:28:50.781635Z","iopub.execute_input":"2022-10-13T01:28:50.782668Z","iopub.status.idle":"2022-10-13T01:28:51.055808Z","shell.execute_reply.started":"2022-10-13T01:28:50.782622Z","shell.execute_reply":"2022-10-13T01:28:51.054517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrtp.hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:20:18.833927Z","iopub.execute_input":"2022-10-13T01:20:18.834339Z","iopub.status.idle":"2022-10-13T01:20:19.268422Z","shell.execute_reply.started":"2022-10-13T01:20:18.834308Z","shell.execute_reply":"2022-10-13T01:20:19.267284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_sqrtp = FunctionTransformer(sqrt_transform)\ntransformed_sqrtp = transformer_sqrtp.fit_transform(y_train[y_train['price'] < y_train['price'].quantile(.99)])","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:20:22.253871Z","iopub.execute_input":"2022-10-13T01:20:22.254277Z","iopub.status.idle":"2022-10-13T01:20:22.269867Z","shell.execute_reply.started":"2022-10-13T01:20:22.254247Z","shell.execute_reply":"2022-10-13T01:20:22.268253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_sqrtp.hist(bins=50) ","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:20:22.934956Z","iopub.execute_input":"2022-10-13T01:20:22.935785Z","iopub.status.idle":"2022-10-13T01:20:23.266298Z","shell.execute_reply.started":"2022-10-13T01:20:22.935740Z","shell.execute_reply":"2022-10-13T01:20:23.265476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exponential Transforms (needs work)\nA close cousin of the log transform is the exponential transformation. Anytime you apply a log transform to your dataset, you can apply an exponential transformation to revert it back to the original value. ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exp_transform(x):\n    return np.exp(x)\n\ntransformer_exp = FunctionTransformer(exp_transform)\ntransformed_exp = transformer_exp.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:54:33.813443Z","iopub.execute_input":"2022-10-13T01:54:33.814125Z","iopub.status.idle":"2022-10-13T01:54:34.091436Z","shell.execute_reply.started":"2022-10-13T01:54:33.814084Z","shell.execute_reply":"2022-10-13T01:54:34.090340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_exp['odometer']","metadata":{"execution":{"iopub.status.busy":"2022-10-13T01:55:06.310082Z","iopub.execute_input":"2022-10-13T01:55:06.310736Z","iopub.status.idle":"2022-10-13T01:55:06.319627Z","shell.execute_reply.started":"2022-10-13T01:55:06.310700Z","shell.execute_reply":"2022-10-13T01:55:06.318810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Box-Cox Transformation\nThe Box-Cox transformation is a transformation that helps your dataset follow a normal distribution. The benefit is that this opens up a number of additional tests you can run on your data. \n\nBox-Cox is an interesting transformation because it essentially aggregates multiple power transformations into a single transformer. You use lambda to adjust the transformation. Lambda varies from -5 to 5. ","metadata":{}},{"cell_type":"code","source":"X_train.min()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T02:01:01.233985Z","iopub.execute_input":"2022-10-13T02:01:01.234522Z","iopub.status.idle":"2022-10-13T02:01:01.262773Z","shell.execute_reply.started":"2022-10-13T02:01:01.234474Z","shell.execute_reply":"2022-10-13T02:01:01.261251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\ntransformer_bc = PowerTransformer(method ='box-cox')\ntransformed_bc = transformer_bc.fit_transform(X_train)\n\nX_train.min()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T02:00:55.822898Z","iopub.execute_input":"2022-10-13T02:00:55.823350Z","iopub.status.idle":"2022-10-13T02:00:55.926400Z","shell.execute_reply.started":"2022-10-13T02:00:55.823317Z","shell.execute_reply":"2022-10-13T02:00:55.925191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like a chef remixing their ingredients, as a data scientist, we have a ton of different ways we can engineer features with our variables. Here are a few common methods:\n\n- Arethmetic Interaction (addition, subtraction, division, or multiplication of variables)\n- Binning (grouping variables in ranges)\n- Creative Features (alternative metrics for evaluation)","metadata":{}}]}